{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4289678,"sourceType":"datasetVersion","datasetId":2527538}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/faryalrifaz3374/diabetes-classification-eda-5-ml-models?scriptVersionId=269193596\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## **Author: Faryal Rifaz**\n\n---\n\n# **Diabetes classification using machine learning models**\n\n---\n\n## Stay Connected!\n\n If you found this notebook helpful or interesting, don't forget to **upvote** and **follow** me for more beginner-friendly, practical notebooks.\n \n#### Letâ€™s connect and grow together on this learning journey.\n\n\n\n### Email: faryalrifaz3374@gmail.com\n### [LinkedIn](https://www.linkedin.com/in/faryal-rifaz-b8a885304/)\n### [Github](https://github.com/Faryalrifaz)\n","metadata":{}},{"cell_type":"markdown","source":"![Diabetes](https://cdn.pixabay.com/photo/2014/11/12/19/25/diabetes-528678__480.jpg \"Understanding Diabetes\")\n","metadata":{}},{"cell_type":"markdown","source":"#  Step 1: Import libraries","metadata":{}},{"cell_type":"markdown","source":"First of all we need to import all the necessary libraries. ","metadata":{}},{"cell_type":"code","source":"# Data handling\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model selection and evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n\n# ML Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.185737Z","iopub.execute_input":"2025-10-15T16:50:31.186172Z","iopub.status.idle":"2025-10-15T16:50:31.192304Z","shell.execute_reply.started":"2025-10-15T16:50:31.186137Z","shell.execute_reply":"2025-10-15T16:50:31.191031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Import Dataset","metadata":{}},{"cell_type":"markdown","source":"Now we will load the diabetes dataset and take a look at its structure to understand the columns, data types, and first few records\n","metadata":{}},{"cell_type":"code","source":"\ndata = pd.read_csv('/kaggle/input/diabetes-dataset/diabetes.csv')\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.19386Z","iopub.execute_input":"2025-10-15T16:50:31.194325Z","iopub.status.idle":"2025-10-15T16:50:31.245995Z","shell.execute_reply.started":"2025-10-15T16:50:31.194268Z","shell.execute_reply":"2025-10-15T16:50:31.244934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3: Check info & missing values","metadata":{}},{"cell_type":"markdown","source":"Now let's check the quick info and see how many missing values are the data set .","metadata":{}},{"cell_type":"code","source":"print(data.info())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.248166Z","iopub.execute_input":"2025-10-15T16:50:31.24856Z","iopub.status.idle":"2025-10-15T16:50:31.260427Z","shell.execute_reply.started":"2025-10-15T16:50:31.248529Z","shell.execute_reply":"2025-10-15T16:50:31.25892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.262002Z","iopub.execute_input":"2025-10-15T16:50:31.262417Z","iopub.status.idle":"2025-10-15T16:50:31.282303Z","shell.execute_reply.started":"2025-10-15T16:50:31.262373Z","shell.execute_reply":"2025-10-15T16:50:31.281177Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 4: Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\n# 1. Replace zero values with median (for specific columns)\ncols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor col in cols:\n    data[col] = data[col].replace(0, data[col].median())\n\n# 2. Split features and target\nX = data.drop('Outcome', axis=1)\ny = data['Outcome']\n\n# 3. Scale features for better model performance\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 4. Balance the dataset using SMOTE\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X_scaled, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.28363Z","iopub.execute_input":"2025-10-15T16:50:31.284067Z","iopub.status.idle":"2025-10-15T16:50:31.31831Z","shell.execute_reply.started":"2025-10-15T16:50:31.284026Z","shell.execute_reply":"2025-10-15T16:50:31.317297Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 5: Get Basic Statistics","metadata":{}},{"cell_type":"markdown","source":"let's have a view of the Basic Statistics ","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.319281Z","iopub.execute_input":"2025-10-15T16:50:31.319552Z","iopub.status.idle":"2025-10-15T16:50:31.352957Z","shell.execute_reply.started":"2025-10-15T16:50:31.319528Z","shell.execute_reply":"2025-10-15T16:50:31.351848Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 5:  Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"let's have some Distributions and patterns from data.","metadata":{}},{"cell_type":"markdown","source":"### **1. Distribution of Outcome** ","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Outcome', data=data)\nplt.title('Distribution of Diabetes Outcome')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.5371Z","iopub.execute_input":"2025-10-15T16:50:31.537491Z","iopub.status.idle":"2025-10-15T16:50:31.69067Z","shell.execute_reply.started":"2025-10-15T16:50:31.537456Z","shell.execute_reply":"2025-10-15T16:50:31.689921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The distribution of the diabetes outcome shows the count of patients with (1) and without (0) diabetes.\n","metadata":{}},{"cell_type":"markdown","source":"### **2. Correlation Matrix**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:31.691411Z","iopub.execute_input":"2025-10-15T16:50:31.691643Z","iopub.status.idle":"2025-10-15T16:50:32.15118Z","shell.execute_reply.started":"2025-10-15T16:50:31.691622Z","shell.execute_reply":"2025-10-15T16:50:32.150104Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3. Distribution of numerical features**","metadata":{}},{"cell_type":"code","source":"num_cols = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\ndata[num_cols].hist(figsize=(12,10), bins=20)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:32.152796Z","iopub.execute_input":"2025-10-15T16:50:32.153112Z","iopub.status.idle":"2025-10-15T16:50:33.521696Z","shell.execute_reply.started":"2025-10-15T16:50:32.153086Z","shell.execute_reply":"2025-10-15T16:50:33.520483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4. Boxplots to check outliers**\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nfor i, col in enumerate(num_cols,1):\n    plt.subplot(3,3,i)\n    sns.boxplot(y=data[col])\n    plt.title(f'Boxplot of {col}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:33.523674Z","iopub.execute_input":"2025-10-15T16:50:33.524163Z","iopub.status.idle":"2025-10-15T16:50:34.886443Z","shell.execute_reply.started":"2025-10-15T16:50:33.524111Z","shell.execute_reply":"2025-10-15T16:50:34.885355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 6: Split the data into X and y","metadata":{}},{"cell_type":"code","source":"X = data.drop('Outcome', axis=1)\ny = data['Outcome']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:34.88755Z","iopub.execute_input":"2025-10-15T16:50:34.887917Z","iopub.status.idle":"2025-10-15T16:50:34.894475Z","shell.execute_reply.started":"2025-10-15T16:50:34.887879Z","shell.execute_reply":"2025-10-15T16:50:34.893149Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 7: Split the data into training and testing sets","metadata":{}},{"cell_type":"markdown","source":"Let's split the data for training and testing.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:34.895587Z","iopub.execute_input":"2025-10-15T16:50:34.89591Z","iopub.status.idle":"2025-10-15T16:50:34.919591Z","shell.execute_reply.started":"2025-10-15T16:50:34.895879Z","shell.execute_reply":"2025-10-15T16:50:34.918434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Step 8: Apply ML Models","metadata":{}},{"cell_type":"markdown","source":"Now it's time to apply Machine Learning models.","metadata":{}},{"cell_type":"code","source":"models = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n    \"Naive Bayes\": GaussianNB(),\n    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:34.920752Z","iopub.execute_input":"2025-10-15T16:50:34.921167Z","iopub.status.idle":"2025-10-15T16:50:34.940228Z","shell.execute_reply.started":"2025-10-15T16:50:34.921126Z","shell.execute_reply":"2025-10-15T16:50:34.939085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 9: Evaluate the models","metadata":{}},{"cell_type":"markdown","source":"let's evaluate the models.","metadata":{}},{"cell_type":"code","source":"results = []\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(f\"Model: {name}\")\n    print(f\"Accuracy: {acc:.4f}\")\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    print(\"-\"*60)\n    results.append([name, acc])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:34.942902Z","iopub.execute_input":"2025-10-15T16:50:34.943323Z","iopub.status.idle":"2025-10-15T16:50:47.747157Z","shell.execute_reply.started":"2025-10-15T16:50:34.943291Z","shell.execute_reply":"2025-10-15T16:50:47.744629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display all results together\nresults_df = pd.DataFrame(results, columns=['Model', 'Accuracy'])\nprint(results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:47.749596Z","iopub.execute_input":"2025-10-15T16:50:47.750046Z","iopub.status.idle":"2025-10-15T16:50:47.761615Z","shell.execute_reply.started":"2025-10-15T16:50:47.750006Z","shell.execute_reply":"2025-10-15T16:50:47.75928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's check through some visuals","metadata":{}},{"cell_type":"code","source":"# Bar plot for model comparison\nplt.figure(figsize=(8,5))\nsns.barplot(x='Model', y='Accuracy', data=results_df)\nplt.title('Comparison of Model Accuracies')\nplt.xticks(rotation=45)\nplt.ylim(0,1)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:47.762569Z","iopub.execute_input":"2025-10-15T16:50:47.763096Z","iopub.status.idle":"2025-10-15T16:50:48.038255Z","shell.execute_reply.started":"2025-10-15T16:50:47.762955Z","shell.execute_reply":"2025-10-15T16:50:48.03706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nplt.figure(figsize=(8,6))\n\nfor name, model in models.items():\n    y_prob = model.predict_proba(X_test)[:,1]\n    fpr, tpr, _ = roc_curve(y_test, y_prob)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n\nplt.plot([0,1], [0,1], 'k--')\nplt.title('ROC Curve Comparison')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T16:50:48.039254Z","iopub.execute_input":"2025-10-15T16:50:48.039591Z","iopub.status.idle":"2025-10-15T16:50:48.372271Z","shell.execute_reply.started":"2025-10-15T16:50:48.039551Z","shell.execute_reply":"2025-10-15T16:50:48.370898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 10: Conclusion","metadata":{}},{"cell_type":"markdown","source":"Among the five models applied on the diabetes dataset, the Random Forest Classifier achieved the highest accuracy of 0.77, followed by SVM (0.74) and Naive Bayes (0.73).\nThe KNN performed the lowest with 0.67, while MLP reached 0.70.\nOverall, Random Forest proved most suitable for diabetes prediction due to its ability to handle complex features and reduce overfitting.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}}]}